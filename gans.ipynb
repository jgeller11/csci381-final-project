{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "GANs are a unique type of neural network that pits two models against each other in order to ultimately create a model that can generate novel output when provided random noise. The \"discriminator\" model has the job of taking an input image and guessing whether it's a real image from the dataset or a generated image. The \"generator\" model takes a noise vector as input and outputs an image, which should hopefully look like the images in the dataset!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Architecture](model_architecture.png \"A schematic showing the model architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch, torchvision\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, Sequential, Linear, ReLU, BatchNorm1d, Sigmoid, Flatten, Unflatten, Conv2d\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234567890\n",
    "torch.manual_seed(SEED)\n",
    "# Optionally always use cpu (more efficient for smaller models)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\" \n",
    "# DEVICE = \"cpu\"\n",
    "IMAGE_WIDTH = 28\n",
    "NOISE_SIZE = 49\n",
    "print(f\"Using random seed: {SEED}\")\n",
    "print(f\"Using Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The raw MNIST data is stored in a [B, H, W] int8 tensor, so we need to convert and rescale the data so each pixel value lies between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloading\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        mnist_data = torchvision.datasets.MNIST('data/mnist', \n",
    "                                        download=True)\n",
    "        self.data = mnist_data.data.to(dtype=torch.float32, device=DEVICE) / 255\n",
    "        self.data = self.data.flatten(1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading Preview\n",
    "\n",
    "Take a look at the images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tensor(tensor):\n",
    "    images = tensor.cpu().reshape(-1, IMAGE_WIDTH, IMAGE_WIDTH).detach().numpy()\n",
    "    _, ax = plt.subplots(ncols=len(images))\n",
    "    for i, image in enumerate(images):\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "        ax[i].imshow(image, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = MNISTDataset()\n",
    "display_tensor(mnist_data[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Discriminator\n",
    "Before we can start generating images, we need to make our discriminator, a model which will predict whether an input image is from the MNIST dataset or was generated some other way. We'll be dealing with large models, so we'll define a ResidualLayer module to make it easy to implement residual connections for speeding up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(Module):\n",
    "    def __init__(self, sequential):\n",
    "        super().__init__()\n",
    "        self.sequential = sequential\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.sequential(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines the architecture of the discriminator. Feel free to play around with it if you want to see how things change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "def create_discriminator():\n",
    "    return Sequential(\n",
    "            ResidualLayer(Sequential(\n",
    "                BatchNorm1d(784), \n",
    "                Linear(784, 784),       \n",
    "                ReLU()\n",
    "            )),\n",
    "            BatchNorm1d(784),\n",
    "            Linear(784, 392),\n",
    "            ReLU(),\n",
    "            ResidualLayer(Sequential(\n",
    "                BatchNorm1d(392),\n",
    "                Linear(392, 392),\n",
    "                ReLU()\n",
    "            )),\n",
    "            BatchNorm1d(392),\n",
    "            Linear(392, 1),\n",
    "            Sigmoid()\n",
    "        ).to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an alternative discriminator with a convolutional neural network (CNN) architecture. You can plug it in later to see how its results differ, but most notably, it's a bit slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative Discriminator Architecture\n",
    "def create_cnn_discriminator():\n",
    "    return Sequential(\n",
    "            Unflatten(1, (1, IMAGE_WIDTH, IMAGE_WIDTH)),\n",
    "            Conv2d(1, 16, kernel_size=7, padding=3),\n",
    "            ReLU(),\n",
    "            Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            ReLU(),\n",
    "            Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            ReLU(),\n",
    "            Flatten(),\n",
    "            BatchNorm1d(64 * IMAGE_WIDTH * IMAGE_WIDTH),\n",
    "            Linear(64 * IMAGE_WIDTH * IMAGE_WIDTH, IMAGE_WIDTH * IMAGE_WIDTH),\n",
    "            ReLU(),\n",
    "            BatchNorm1d(IMAGE_WIDTH * IMAGE_WIDTH),\n",
    "            Linear(IMAGE_WIDTH * IMAGE_WIDTH, 1),\n",
    "            Sigmoid()\n",
    "        ).to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a loss function, we use the negative log likelihood of the discriminator making a correct prediction. To take a training step for the discriminator, we'll pass it a variety of images and labels indicating whether each image is from the MNIST dataset (1) or from our generator model (0). We can then calculate the loss, perform backpropagation, and take a gradient descent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss_fn(predictions, labels, epsilon = 1e-5):\n",
    "    return -torch.mean(torch.log(epsilon + 1 - torch.abs(predictions - labels))) + epsilon\n",
    "\n",
    "# Train Discriminator    \n",
    "def discriminator_train_step(discriminator, real_images, fake_images, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    images = torch.cat([real_images, fake_images])\n",
    "\n",
    "    # Create labels for the samples\n",
    "    labels = torch.cat((torch.ones(len(real_images), device=DEVICE), \n",
    "                        torch.zeros(len(fake_images), device=DEVICE)))\n",
    "    \n",
    "    # Compute predictions and update the parameters\n",
    "    predictions = discriminator(images).squeeze(dim = 1)\n",
    "    loss = discriminator_loss_fn(predictions, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test, let's generate some images which are just noise and feed them to the discriminator to see if it can learn to distinguish complete noise from actual MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_images = torch.randn(5, IMAGE_WIDTH, IMAGE_WIDTH)\n",
    "display_tensor(fake_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "discriminator = create_discriminator()\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr = 0.0002, betas=(0.5, 0.9))\n",
    "for real_images in DataLoader(mnist_data, batch_size=256):\n",
    "    # Generate random noise to train against\n",
    "    fake_images = torch.randn_like(real_images)\n",
    "\n",
    "    loss = discriminator_train_step(discriminator, real_images, fake_images, discriminator_optimizer)\n",
    "    print(\"Loss: {:10.7f}\".format(loss), end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the loss get very small very quickly: we've learned to distinguish between them! Now that we have a way of benchmarking our generator model, we're ready to start training it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Generator\n",
    "\n",
    "We'll now create a feed-forward network to serve as the generator. Generating new images turns out to be a more complicated task than classifying them, so this network will be much larger than the discriminator. It takes as input a 49-element vector containing complete noise, and outputs a 784-element vector with elements between 0 and 1 which can be reshaped into an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "def create_generator():\n",
    "    return Sequential(\n",
    "            ResidualLayer(Sequential(\n",
    "                BatchNorm1d(49),\n",
    "                Linear(49, 49),\n",
    "                ReLU()\n",
    "            )),\n",
    "            BatchNorm1d(49),\n",
    "            Linear(49, 98),\n",
    "            ReLU(),\n",
    "            ResidualLayer(Sequential(\n",
    "                BatchNorm1d(98),\n",
    "                Linear(98, 98),\n",
    "                ReLU()\n",
    "            )),\n",
    "            BatchNorm1d(98),\n",
    "            Linear(98, 196),\n",
    "            ReLU(),\n",
    "            ResidualLayer(Sequential(\n",
    "                BatchNorm1d(196),\n",
    "                Linear(196, 196),\n",
    "                ReLU()\n",
    "            )),\n",
    "            BatchNorm1d(196),\n",
    "            Linear(196, 392),\n",
    "            ReLU(),\n",
    "            ResidualLayer(Sequential(\n",
    "                BatchNorm1d(392),\n",
    "                Linear(392, 392),\n",
    "                ReLU()\n",
    "            )),\n",
    "            BatchNorm1d(392),\n",
    "            Linear(392, 784),\n",
    "            ResidualLayer(Sequential(\n",
    "                BatchNorm1d(784),\n",
    "                Linear(784, 784),\n",
    "                ReLU()\n",
    "            )),\n",
    "            Sigmoid()\n",
    "        ).to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a loss function, we use the negative log likelihood of the discriminator making an *incorrect* prediction, since the generator's goal is to fool the discriminator as often as possible. To take a training step for the generator, we'll pass it a batch of noise vectors, have it generate images, then send those images to the discriminator for classification. Based on the output of that model, we calculate this loss function, then perform backpropagation all the way back to the generator and take a gradient descent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss_fn(predictions, epsilon = 1e-5):\n",
    "    return -torch.mean(torch.log(epsilon + predictions))\n",
    "\n",
    "# Train Generator\n",
    "def generator_train_step(generator, discriminator, batch_size, generator_optimizer, discriminator_optimizer):\n",
    "    generator_optimizer.zero_grad()\n",
    "    discriminator_optimizer.zero_grad()\n",
    "\n",
    "    noise = torch.randn(batch_size, NOISE_SIZE, device=DEVICE)\n",
    "    generated_images = generator(noise)\n",
    "    \n",
    "    # Compute predictions and update the parameters\n",
    "    predictions = discriminator(generated_images).squeeze(dim = 1)\n",
    "    loss = generator_loss_fn(predictions)\n",
    "    loss.backward()\n",
    "    generator_optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our Model! But first we need to define some evaluation functions, so we can see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate(generator, discriminator, dataloader):\n",
    "    # Variables for tracking stats as we iterate through the data in batches\n",
    "    total_samples = 0\n",
    "    correct_on_real_images = 0\n",
    "    correct_on_generated_images = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # Get number correct on real images\n",
    "        real_preds = discriminator(batch).squeeze(dim=1)\n",
    "        correct_on_real_images += (real_preds > 0.5).sum()\n",
    "\n",
    "        # Get number correct on generated images\n",
    "        noise = torch.randn(len(batch), NOISE_SIZE, device=DEVICE)\n",
    "        generated_preds = discriminator(generator(noise)).squeeze(dim=1)\n",
    "        correct_on_generated_images += (generated_preds < 0.5).sum()\n",
    "        \n",
    "        # Track how many images we've seen\n",
    "        total_samples += len(batch)\n",
    "    \n",
    "    # Calculate Accuracies\n",
    "    real_correct_acc = correct_on_real_images / total_samples\n",
    "    generated_correct_acc = correct_on_generated_images / total_samples\n",
    "\n",
    "    return real_correct_acc, generated_correct_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can train! For 10 epochs (depending on your hardware, each epoch should take ~30 seconds), we take minibatches of 64 data images, take a step training the discriminator on 64 real images and 64 generated images from the generator, then take a step training the generator on 64 noise inputs. You can watch the accuracy of the discriminator on both data and generated images change from epoch to epoch, and also see a sample of generated images from the generator at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "discriminator = create_discriminator() # Try replacing this with: create_cnn_discriminator()\n",
    "generator = create_generator()\n",
    "\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr = 0.0002, betas=(0.5, 0.9))\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr = 0.0001, betas=(0.5, 0.9))\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "test_noise = torch.randn(6, NOISE_SIZE, device=DEVICE)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(f\"Epoch {1 + epoch}\")\n",
    "    # Training Loop\n",
    "    for real_images in tqdm(DataLoader(mnist_data, batch_size=batch_size)):\n",
    "        # Discriminator training loop\n",
    "        discriminator.train()\n",
    "        generator.eval()\n",
    "\n",
    "        noise = torch.randn(batch_size, NOISE_SIZE, device=DEVICE)\n",
    "        fake_images = generator(noise)\n",
    "        discriminator_train_step(discriminator, real_images, fake_images, discriminator_optimizer)\n",
    "\n",
    "        # Generator training subroutine\n",
    "        discriminator.eval()\n",
    "        generator.train()\n",
    "        generator_train_step(generator, discriminator, batch_size, generator_optimizer, discriminator_optimizer)\n",
    "    \n",
    "    # Evaluate current performance\n",
    "    real_correct_acc, generated_correct_acc = evaluate(generator, discriminator, DataLoader(mnist_data, batch_size=2048))\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    print()\n",
    "    print(f\"Real Image Accuracy:      {real_correct_acc}\")\n",
    "    print(f\"Generated Image Accuracy: {generated_correct_acc}\")\n",
    "    print()\n",
    "    print(\"Example images:\")\n",
    "    display_tensor(generator(test_noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how your seed is set, you might see the generated images as complete noise, you may see every generated image showing different versions of the same number, or if you're lucky, you'll see a wide variety of beautiful numbers! As it turns out, these models are quite sensitive to initial conditions, and so on any given attempt to train the model, there can be a wide variety of results. Feel free to modify the code to run more training epochs, run it with different seeds, or play with the architecture of the model (perhaps, for example, by using the CNN architecture for the discriminator rather than the feed-forward architecture there by default).\n",
    "\n",
    "The phenomenon where most of the generated images converge to being the same one or two digits is a fairly common one, referred to as \"mode collapse.\" There are difficulties getting the model to learn several discrete categories simultaneously without also producing images somehow \"between\" those categories, which will be recognized as not being numbers, and thus be caught easily by the discriminator. Therefore, the generator can be punished heavily for generating too diverse a spread of images, and so it often learns to be good at producing just a couple specific digits well, rather than all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![And that's it! You may now submit lab 10.](final_image.png \"And that's it! You may now submit lab 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "- [Original GAN paper](https://arxiv.org/pdf/1406.2661)\n",
    "- [Useful article discussing ways of troubleshooting GAN models](https://jonathan-hui.medium.com/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b)\n",
    "- [A little more about mode collapse](https://neptune.ai/blog/gan-loss-functions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs381",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
